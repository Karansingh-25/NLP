{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef0629d-68dc-4812-bb67-e752640b3e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Requirement already satisfied: nltk in c:\\users\\karan\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\karan\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\karan\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\karan\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\karan\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5e929b-d0f0-4bc6-8192-d85e685a8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"Hello Welcome, to Tokenization's Notebook.\n",
    "    It requires some Pre-Requisites like Python as a programming language.\n",
    "    I am very excited! to learn ahead.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1bec8d8-d66f-4a2c-a356-6cc3dadfd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Tokenization's Notebook.\n",
      "    It requires some Pre-Requisites like Python as a programming language.\n",
      "    I am very excited! to learn ahead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7fb3c-8c67-4d3c-a7b7-769a810e5844",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fadf3f8-76bb-4fe5-afb4-087e6d56802a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b315785-7f8a-4a2e-be67-92ce580958bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences -> paragraphs\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b264ca8-3139-4afe-8219-08893bf8d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf399d60-ccd0-478d-9df5-cdb7157ee8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c9b78f-0478-4276-8680-b03928a531db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to Tokenization's Notebook.\n",
      "It requires some Pre-Requisites like Python as a programming language.\n",
      "I am very excited!\n",
      "to learn ahead.\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06af35e6-a7bb-45a2-88b5-45ca0ef395be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence -> words\n",
    "word_doc=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73acd96f-e4ef-4a5a-8f7c-585dd913af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e85b54d-4ce1-423e-925c-93ca5001b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Tokenization', \"'s\", 'Notebook', '.']\n",
      "['It', 'requires', 'some', 'Pre-Requisites', 'like', 'Python', 'as', 'a', 'programming', 'language', '.']\n",
      "['I', 'am', 'very', 'excited', '!']\n",
      "['to', 'learn', 'ahead', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e329b8-d107-4598-81b3-22305c4ca55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits up the \" 's \" also\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148921b3-5d02-49bc-ae57-c688670dea8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Tokenization', \"'\", 's', 'Notebook', '.']\n",
      "['It', 'requires', 'some', 'Pre', '-', 'Requisites', 'like', 'Python', 'as', 'a', 'programming', 'language', '.']\n",
      "['I', 'am', 'very', 'excited', '!']\n",
      "['to', 'learn', 'ahead', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(wordpunct_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d645e9e1-cf3b-4495-afa8-dc32f168108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ea7253-6897-45ba-bd52-c3729e74f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e37e4a-4582-4231-aab8-449a25286a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Welcome',\n",
       " ',',\n",
       " 'to',\n",
       " 'Tokenization',\n",
       " \"'s\",\n",
       " 'Notebook.',\n",
       " 'It',\n",
       " 'requires',\n",
       " 'some',\n",
       " 'Pre-Requisites',\n",
       " 'like',\n",
       " 'Python',\n",
       " 'as',\n",
       " 'a',\n",
       " 'programming',\n",
       " 'language.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'very',\n",
       " 'excited',\n",
       " '!',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'ahead',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#full stop will not be used as a seperate word except the last full stop\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d32fcdc4-cd74-40a8-9d6f-e216b313ef5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Welcome', ',', 'to', 'Tokenization', \"'s\", 'Notebook', '.']\n",
      "['It', 'requires', 'some', 'Pre-Requisites', 'like', 'Python', 'as', 'a', 'programming', 'language', '.']\n",
      "['I', 'am', 'very', 'excited', '!']\n",
      "['to', 'learn', 'ahead', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print(tokenizer.tokenize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbda42d-e59b-4d32-95e6-7ccdeafd513a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ca0b73e-f925-41ee-bea4-9c4da865111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"playing\",\"played\",\"plays\",\"happily\",\"running\",\"eating\",\"eaten\",\"eats\",\"run\",\"happy\",\"runned\",\"run\",\"history\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e76575-020c-45c6-890e-3548726d2456",
   "metadata": {},
   "source": [
    "## 1. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d42500-222c-456d-bd35-ce4cecdbdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b34c9ee1-71de-4445-9cee-773adeb88bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b65486c6-3015-4b14-95da-ce60d4fe1dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing------>play\n",
      "played------>play\n",
      "plays------>play\n",
      "happily------>happili\n",
      "running------>run\n",
      "eating------>eat\n",
      "eaten------>eaten\n",
      "eats------>eat\n",
      "run------>run\n",
      "happy------>happi\n",
      "runned------>run\n",
      "run------>run\n",
      "history------>histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157720e2-3279-4c2c-aeb2-2305aa19e473",
   "metadata": {},
   "source": [
    "## 2. RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f72b4cf2-554f-4548-80f0-1a7688a18396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c75c212-8390-410b-b79f-bb18c3046350",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stemmer=RegexpStemmer(\"ing$|s$|ed$|il$|able$\",min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92f65662-144d-40cc-b6c5-9861be13e9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"eating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88af647b-d342-4896-ba01-9e58a5144ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happily'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_stemmer.stem(\"happily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee47bc-f388-46bc-bf8a-0066862d5097",
   "metadata": {},
   "source": [
    "## 3. SnowBall Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "804aaadc-d6c1-4f55-9b35-85e6e5cd7770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77d4a802-9b7d-4ef4-9fe2-bf34b50ec4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "461e5c7b-b5d2-4472-9649-53fea2d14a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing------>play\n",
      "played------>play\n",
      "plays------>play\n",
      "happily------>happili\n",
      "running------>run\n",
      "eating------>eat\n",
      "eaten------>eaten\n",
      "eats------>eat\n",
      "run------>run\n",
      "happy------>happi\n",
      "runned------>run\n",
      "run------>run\n",
      "history------>histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+snowball.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f402211a-d584-4682-a6aa-1f9eb094e9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli', 'superfici')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(\"fairly\"),ps.stem(\"sportingly\"),ps.stem(\"superficially\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b0c129c-5df1-4227-9de6-a5cd842c6dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport', 'superfici')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball.stem(\"fairly\"),snowball.stem(\"sportingly\"),snowball.stem(\"superficially\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cd30e-9085-4fb1-92a6-29e47aef4f65",
   "metadata": {},
   "source": [
    "# LemmaTization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587e949f-1263-434d-b6bf-e8b379ae3695",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "611f1f0b-1eb2-4985-b387-903edfe884bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20975879-e199-4b56-8fe4-1ea20858ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24c99086-757b-4b96-823c-631f3b4601fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad6454-2f23-4d70-85df-21f54431de84",
   "metadata": {},
   "source": [
    "### Pos\n",
    "Noun - n\n",
    "verb - v\n",
    "adjective - a\n",
    "adverb - r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d57a8a8-25e2-4996-b909-a1a8fb2308d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\",pos=\"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca84e29c-c10d-4ac0-8abc-58c49e4ca908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"running\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c097a27-3666-42b3-8a04-04697c9c3aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing------>playing\n",
      "played------>played\n",
      "plays------>plays\n",
      "happily------>happily\n",
      "running------>running\n",
      "eating------>eating\n",
      "eaten------>eaten\n",
      "eats------>eats\n",
      "run------>run\n",
      "happy------>happy\n",
      "runned------>runned\n",
      "run------>run\n",
      "history------>history\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\"------>\"+lemmatizer.lemmatize(word,pos=\"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174ac32-ddbc-42d6-85fc-05204f93686c",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f8f7fb0-3d39-42bc-bfa1-24fc997e6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='''\n",
    "In my words, I am part of an ancient legacy that stretches back to the dawn of human civilization. The great\n",
    "strides we have made together are the testament to our unyielding spirit, which has thrived through overcoming\n",
    "obstacles and uniting against adversity. However, this journey is not without its toll; it’s a reminder of how far\n",
    "we’ve come yet how much remains to build an inclusive and prosperous future.\n",
    "\n",
    "The challenges we face today require both strength and hope. The struggles in areas like poverty, education, and\n",
    "infrastructure continue to stand as reminders of the human condition—a struggle for equality and justice that we\n",
    "strive to honor but cannot ignore. Yet, it is through unity and shared purpose that we can overcome these\n",
    "obstacles.\n",
    "\n",
    "This narrative of resilience reminds us that, despite our difficulties, there is still hope. The spirit of hope we\n",
    "cultivate today continues to inspire those around us, fostering a sense of unity and camaraderie. Together, we\n",
    "stand on the brink of new opportunities—opportunity for progress, opportunity for inspiration—and the potential\n",
    "for a better future.\n",
    "\n",
    "As we move forward, let us keep in mind that the seeds of change are planted in every individual’s hearts.\n",
    "Together, we can create a future where unity reigns supreme and hope prevails as strength. Let us carry this\n",
    "vision into all that we do—her marching bands, our schools, or even the streets of our cities—to inspire others to\n",
    "join in the ongoing struggle for a better world.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6cf6726-0686-4cda-9914-7c66ab15841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25807b2d-8be2-4785-97ef-b96d9a38354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef3fd015-aa48-465d-bc42-9f667cd42517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7d0989a-2fa5-4f74-b171-1c350e17ace6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c32d9d10-e2ed-41d2-856a-715d32d6c6df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إذ',\n",
       " 'إذا',\n",
       " 'إذما',\n",
       " 'إذن',\n",
       " 'أف',\n",
       " 'أقل',\n",
       " 'أكثر',\n",
       " 'ألا',\n",
       " 'إلا',\n",
       " 'التي',\n",
       " 'الذي',\n",
       " 'الذين',\n",
       " 'اللاتي',\n",
       " 'اللائي',\n",
       " 'اللتان',\n",
       " 'اللتيا',\n",
       " 'اللتين',\n",
       " 'اللذان',\n",
       " 'اللذين',\n",
       " 'اللواتي',\n",
       " 'إلى',\n",
       " 'إليك',\n",
       " 'إليكم',\n",
       " 'إليكما',\n",
       " 'إليكن',\n",
       " 'أم',\n",
       " 'أما',\n",
       " 'أما',\n",
       " 'إما',\n",
       " 'أن',\n",
       " 'إن',\n",
       " 'إنا',\n",
       " 'أنا',\n",
       " 'أنت',\n",
       " 'أنتم',\n",
       " 'أنتما',\n",
       " 'أنتن',\n",
       " 'إنما',\n",
       " 'إنه',\n",
       " 'أنى',\n",
       " 'أنى',\n",
       " 'آه',\n",
       " 'آها',\n",
       " 'أو',\n",
       " 'أولاء',\n",
       " 'أولئك',\n",
       " 'أوه',\n",
       " 'آي',\n",
       " 'أي',\n",
       " 'أيها',\n",
       " 'إي',\n",
       " 'أين',\n",
       " 'أين',\n",
       " 'أينما',\n",
       " 'إيه',\n",
       " 'بخ',\n",
       " 'بس',\n",
       " 'بعد',\n",
       " 'بعض',\n",
       " 'بك',\n",
       " 'بكم',\n",
       " 'بكم',\n",
       " 'بكما',\n",
       " 'بكن',\n",
       " 'بل',\n",
       " 'بلى',\n",
       " 'بما',\n",
       " 'بماذا',\n",
       " 'بمن',\n",
       " 'بنا',\n",
       " 'به',\n",
       " 'بها',\n",
       " 'بهم',\n",
       " 'بهما',\n",
       " 'بهن',\n",
       " 'بي',\n",
       " 'بين',\n",
       " 'بيد',\n",
       " 'تلك',\n",
       " 'تلكم',\n",
       " 'تلكما',\n",
       " 'ته',\n",
       " 'تي',\n",
       " 'تين',\n",
       " 'تينك',\n",
       " 'ثم',\n",
       " 'ثمة',\n",
       " 'حاشا',\n",
       " 'حبذا',\n",
       " 'حتى',\n",
       " 'حيث',\n",
       " 'حيثما',\n",
       " 'حين',\n",
       " 'خلا',\n",
       " 'دون',\n",
       " 'ذا',\n",
       " 'ذات',\n",
       " 'ذاك',\n",
       " 'ذان',\n",
       " 'ذانك',\n",
       " 'ذلك',\n",
       " 'ذلكم',\n",
       " 'ذلكما',\n",
       " 'ذلكن',\n",
       " 'ذه',\n",
       " 'ذو',\n",
       " 'ذوا',\n",
       " 'ذواتا',\n",
       " 'ذواتي',\n",
       " 'ذي',\n",
       " 'ذين',\n",
       " 'ذينك',\n",
       " 'ريث',\n",
       " 'سوف',\n",
       " 'سوى',\n",
       " 'شتان',\n",
       " 'عدا',\n",
       " 'عسى',\n",
       " 'عل',\n",
       " 'على',\n",
       " 'عليك',\n",
       " 'عليه',\n",
       " 'عما',\n",
       " 'عن',\n",
       " 'عند',\n",
       " 'غير',\n",
       " 'فإذا',\n",
       " 'فإن',\n",
       " 'فلا',\n",
       " 'فمن',\n",
       " 'في',\n",
       " 'فيم',\n",
       " 'فيما',\n",
       " 'فيه',\n",
       " 'فيها',\n",
       " 'قد',\n",
       " 'كأن',\n",
       " 'كأنما',\n",
       " 'كأي',\n",
       " 'كأين',\n",
       " 'كذا',\n",
       " 'كذلك',\n",
       " 'كل',\n",
       " 'كلا',\n",
       " 'كلاهما',\n",
       " 'كلتا',\n",
       " 'كلما',\n",
       " 'كليكما',\n",
       " 'كليهما',\n",
       " 'كم',\n",
       " 'كم',\n",
       " 'كما',\n",
       " 'كي',\n",
       " 'كيت',\n",
       " 'كيف',\n",
       " 'كيفما',\n",
       " 'لا',\n",
       " 'لاسيما',\n",
       " 'لدى',\n",
       " 'لست',\n",
       " 'لستم',\n",
       " 'لستما',\n",
       " 'لستن',\n",
       " 'لسن',\n",
       " 'لسنا',\n",
       " 'لعل',\n",
       " 'لك',\n",
       " 'لكم',\n",
       " 'لكما',\n",
       " 'لكن',\n",
       " 'لكنما',\n",
       " 'لكي',\n",
       " 'لكيلا',\n",
       " 'لم',\n",
       " 'لما',\n",
       " 'لن',\n",
       " 'لنا',\n",
       " 'له',\n",
       " 'لها',\n",
       " 'لهم',\n",
       " 'لهما',\n",
       " 'لهن',\n",
       " 'لو',\n",
       " 'لولا',\n",
       " 'لوما',\n",
       " 'لي',\n",
       " 'لئن',\n",
       " 'ليت',\n",
       " 'ليس',\n",
       " 'ليسا',\n",
       " 'ليست',\n",
       " 'ليستا',\n",
       " 'ليسوا',\n",
       " 'ما',\n",
       " 'ماذا',\n",
       " 'متى',\n",
       " 'مذ',\n",
       " 'مع',\n",
       " 'مما',\n",
       " 'ممن',\n",
       " 'من',\n",
       " 'منه',\n",
       " 'منها',\n",
       " 'منذ',\n",
       " 'مه',\n",
       " 'مهما',\n",
       " 'نحن',\n",
       " 'نحو',\n",
       " 'نعم',\n",
       " 'ها',\n",
       " 'هاتان',\n",
       " 'هاته',\n",
       " 'هاتي',\n",
       " 'هاتين',\n",
       " 'هاك',\n",
       " 'هاهنا',\n",
       " 'هذا',\n",
       " 'هذان',\n",
       " 'هذه',\n",
       " 'هذي',\n",
       " 'هذين',\n",
       " 'هكذا',\n",
       " 'هل',\n",
       " 'هلا',\n",
       " 'هم',\n",
       " 'هما',\n",
       " 'هن',\n",
       " 'هنا',\n",
       " 'هناك',\n",
       " 'هنالك',\n",
       " 'هو',\n",
       " 'هؤلاء',\n",
       " 'هي',\n",
       " 'هيا',\n",
       " 'هيت',\n",
       " 'هيهات',\n",
       " 'والذي',\n",
       " 'والذين',\n",
       " 'وإذ',\n",
       " 'وإذا',\n",
       " 'وإن',\n",
       " 'ولا',\n",
       " 'ولكن',\n",
       " 'ولو',\n",
       " 'وما',\n",
       " 'ومن',\n",
       " 'وهو',\n",
       " 'يا',\n",
       " 'أبٌ',\n",
       " 'أخٌ',\n",
       " 'حمٌ',\n",
       " 'فو',\n",
       " 'أنتِ',\n",
       " 'يناير',\n",
       " 'فبراير',\n",
       " 'مارس',\n",
       " 'أبريل',\n",
       " 'مايو',\n",
       " 'يونيو',\n",
       " 'يوليو',\n",
       " 'أغسطس',\n",
       " 'سبتمبر',\n",
       " 'أكتوبر',\n",
       " 'نوفمبر',\n",
       " 'ديسمبر',\n",
       " 'جانفي',\n",
       " 'فيفري',\n",
       " 'مارس',\n",
       " 'أفريل',\n",
       " 'ماي',\n",
       " 'جوان',\n",
       " 'جويلية',\n",
       " 'أوت',\n",
       " 'كانون',\n",
       " 'شباط',\n",
       " 'آذار',\n",
       " 'نيسان',\n",
       " 'أيار',\n",
       " 'حزيران',\n",
       " 'تموز',\n",
       " 'آب',\n",
       " 'أيلول',\n",
       " 'تشرين',\n",
       " 'دولار',\n",
       " 'دينار',\n",
       " 'ريال',\n",
       " 'درهم',\n",
       " 'ليرة',\n",
       " 'جنيه',\n",
       " 'قرش',\n",
       " 'مليم',\n",
       " 'فلس',\n",
       " 'هللة',\n",
       " 'سنتيم',\n",
       " 'يورو',\n",
       " 'ين',\n",
       " 'يوان',\n",
       " 'شيكل',\n",
       " 'واحد',\n",
       " 'اثنان',\n",
       " 'ثلاثة',\n",
       " 'أربعة',\n",
       " 'خمسة',\n",
       " 'ستة',\n",
       " 'سبعة',\n",
       " 'ثمانية',\n",
       " 'تسعة',\n",
       " 'عشرة',\n",
       " 'أحد',\n",
       " 'اثنا',\n",
       " 'اثني',\n",
       " 'إحدى',\n",
       " 'ثلاث',\n",
       " 'أربع',\n",
       " 'خمس',\n",
       " 'ست',\n",
       " 'سبع',\n",
       " 'ثماني',\n",
       " 'تسع',\n",
       " 'عشر',\n",
       " 'ثمان',\n",
       " 'سبت',\n",
       " 'أحد',\n",
       " 'اثنين',\n",
       " 'ثلاثاء',\n",
       " 'أربعاء',\n",
       " 'خميس',\n",
       " 'جمعة',\n",
       " 'أول',\n",
       " 'ثان',\n",
       " 'ثاني',\n",
       " 'ثالث',\n",
       " 'رابع',\n",
       " 'خامس',\n",
       " 'سادس',\n",
       " 'سابع',\n",
       " 'ثامن',\n",
       " 'تاسع',\n",
       " 'عاشر',\n",
       " 'حادي',\n",
       " 'أ',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ك',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ي',\n",
       " 'ء',\n",
       " 'ى',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'أ',\n",
       " 'ة',\n",
       " 'ألف',\n",
       " 'باء',\n",
       " 'تاء',\n",
       " 'ثاء',\n",
       " 'جيم',\n",
       " 'حاء',\n",
       " 'خاء',\n",
       " 'دال',\n",
       " 'ذال',\n",
       " 'راء',\n",
       " 'زاي',\n",
       " 'سين',\n",
       " 'شين',\n",
       " 'صاد',\n",
       " 'ضاد',\n",
       " 'طاء',\n",
       " 'ظاء',\n",
       " 'عين',\n",
       " 'غين',\n",
       " 'فاء',\n",
       " 'قاف',\n",
       " 'كاف',\n",
       " 'لام',\n",
       " 'ميم',\n",
       " 'نون',\n",
       " 'هاء',\n",
       " 'واو',\n",
       " 'ياء',\n",
       " 'همزة',\n",
       " 'ي',\n",
       " 'نا',\n",
       " 'ك',\n",
       " 'كن',\n",
       " 'ه',\n",
       " 'إياه',\n",
       " 'إياها',\n",
       " 'إياهما',\n",
       " 'إياهم',\n",
       " 'إياهن',\n",
       " 'إياك',\n",
       " 'إياكما',\n",
       " 'إياكم',\n",
       " 'إياك',\n",
       " 'إياكن',\n",
       " 'إياي',\n",
       " 'إيانا',\n",
       " 'أولالك',\n",
       " 'تانِ',\n",
       " 'تانِك',\n",
       " 'تِه',\n",
       " 'تِي',\n",
       " 'تَيْنِ',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'ذانِ',\n",
       " 'ذِه',\n",
       " 'ذِي',\n",
       " 'ذَيْنِ',\n",
       " 'هَؤلاء',\n",
       " 'هَاتانِ',\n",
       " 'هَاتِه',\n",
       " 'هَاتِي',\n",
       " 'هَاتَيْنِ',\n",
       " 'هَذا',\n",
       " 'هَذانِ',\n",
       " 'هَذِه',\n",
       " 'هَذِي',\n",
       " 'هَذَيْنِ',\n",
       " 'الألى',\n",
       " 'الألاء',\n",
       " 'أل',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'أنّى',\n",
       " 'أيّ',\n",
       " 'ّأيّان',\n",
       " 'ذيت',\n",
       " 'كأيّ',\n",
       " 'كأيّن',\n",
       " 'بضع',\n",
       " 'فلان',\n",
       " 'وا',\n",
       " 'آمينَ',\n",
       " 'آهِ',\n",
       " 'آهٍ',\n",
       " 'آهاً',\n",
       " 'أُفٍّ',\n",
       " 'أُفٍّ',\n",
       " 'أفٍّ',\n",
       " 'أمامك',\n",
       " 'أمامكَ',\n",
       " 'أوّهْ',\n",
       " 'إلَيْكَ',\n",
       " 'إلَيْكَ',\n",
       " 'إليكَ',\n",
       " 'إليكنّ',\n",
       " 'إيهٍ',\n",
       " 'بخٍ',\n",
       " 'بسّ',\n",
       " 'بَسْ',\n",
       " 'بطآن',\n",
       " 'بَلْهَ',\n",
       " 'حاي',\n",
       " 'حَذارِ',\n",
       " 'حيَّ',\n",
       " 'حيَّ',\n",
       " 'دونك',\n",
       " 'رويدك',\n",
       " 'سرعان',\n",
       " 'شتانَ',\n",
       " 'شَتَّانَ',\n",
       " 'صهْ',\n",
       " 'صهٍ',\n",
       " 'طاق',\n",
       " 'طَق',\n",
       " 'عَدَسْ',\n",
       " 'كِخ',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانَك',\n",
       " 'مكانكم',\n",
       " 'مكانكما',\n",
       " 'مكانكنّ',\n",
       " 'نَخْ',\n",
       " 'هاكَ',\n",
       " 'هَجْ',\n",
       " 'هلم',\n",
       " 'هيّا',\n",
       " 'هَيْهات',\n",
       " 'وا',\n",
       " 'واهاً',\n",
       " 'وراءَك',\n",
       " 'وُشْكَانَ',\n",
       " 'وَيْ',\n",
       " 'يفعلان',\n",
       " 'تفعلان',\n",
       " 'يفعلون',\n",
       " 'تفعلون',\n",
       " 'تفعلين',\n",
       " 'اتخذ',\n",
       " 'ألفى',\n",
       " 'تخذ',\n",
       " 'ترك',\n",
       " 'تعلَّم',\n",
       " 'جعل',\n",
       " 'حجا',\n",
       " 'حبيب',\n",
       " 'خال',\n",
       " 'حسب',\n",
       " 'خال',\n",
       " 'درى',\n",
       " 'رأى',\n",
       " 'زعم',\n",
       " 'صبر',\n",
       " 'ظنَّ',\n",
       " 'عدَّ',\n",
       " 'علم',\n",
       " 'غادر',\n",
       " 'ذهب',\n",
       " 'وجد',\n",
       " 'ورد',\n",
       " 'وهب',\n",
       " 'أسكن',\n",
       " 'أطعم',\n",
       " 'أعطى',\n",
       " 'رزق',\n",
       " 'زود',\n",
       " 'سقى',\n",
       " 'كسا',\n",
       " 'أخبر',\n",
       " 'أرى',\n",
       " 'أعلم',\n",
       " 'أنبأ',\n",
       " 'حدَث',\n",
       " 'خبَّر',\n",
       " 'نبَّا',\n",
       " 'أفعل به',\n",
       " 'ما أفعله',\n",
       " 'بئس',\n",
       " 'ساء',\n",
       " 'طالما',\n",
       " 'قلما',\n",
       " 'لات',\n",
       " 'لكنَّ',\n",
       " 'ءَ',\n",
       " 'أجل',\n",
       " 'إذاً',\n",
       " 'أمّا',\n",
       " 'إمّا',\n",
       " 'إنَّ',\n",
       " 'أنًّ',\n",
       " 'أى',\n",
       " 'إى',\n",
       " 'أيا',\n",
       " 'ب',\n",
       " 'ثمَّ',\n",
       " 'جلل',\n",
       " 'جير',\n",
       " 'رُبَّ',\n",
       " 'س',\n",
       " 'علًّ',\n",
       " 'ف',\n",
       " 'كأنّ',\n",
       " 'كلَّا',\n",
       " 'كى',\n",
       " 'ل',\n",
       " 'لات',\n",
       " 'لعلَّ',\n",
       " 'لكنَّ',\n",
       " 'لكنَّ',\n",
       " 'م',\n",
       " 'نَّ',\n",
       " 'هلّا',\n",
       " 'وا',\n",
       " 'أل',\n",
       " 'إلّا',\n",
       " 'ت',\n",
       " 'ك',\n",
       " 'لمّا',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ا',\n",
       " 'ي',\n",
       " 'تجاه',\n",
       " 'تلقاء',\n",
       " 'جميع',\n",
       " 'حسب',\n",
       " 'سبحان',\n",
       " 'شبه',\n",
       " 'لعمر',\n",
       " 'مثل',\n",
       " 'معاذ',\n",
       " 'أبو',\n",
       " 'أخو',\n",
       " 'حمو',\n",
       " 'فو',\n",
       " 'مئة',\n",
       " 'مئتان',\n",
       " 'ثلاثمئة',\n",
       " 'أربعمئة',\n",
       " 'خمسمئة',\n",
       " 'ستمئة',\n",
       " 'سبعمئة',\n",
       " 'ثمنمئة',\n",
       " 'تسعمئة',\n",
       " 'مائة',\n",
       " 'ثلاثمائة',\n",
       " 'أربعمائة',\n",
       " 'خمسمائة',\n",
       " 'ستمائة',\n",
       " 'سبعمائة',\n",
       " 'ثمانمئة',\n",
       " 'تسعمائة',\n",
       " 'عشرون',\n",
       " 'ثلاثون',\n",
       " 'اربعون',\n",
       " 'خمسون',\n",
       " 'ستون',\n",
       " 'سبعون',\n",
       " 'ثمانون',\n",
       " 'تسعون',\n",
       " 'عشرين',\n",
       " 'ثلاثين',\n",
       " 'اربعين',\n",
       " 'خمسين',\n",
       " 'ستين',\n",
       " 'سبعين',\n",
       " 'ثمانين',\n",
       " 'تسعين',\n",
       " 'بضع',\n",
       " 'نيف',\n",
       " 'أجمع',\n",
       " 'جميع',\n",
       " 'عامة',\n",
       " 'عين',\n",
       " 'نفس',\n",
       " 'لا سيما',\n",
       " 'أصلا',\n",
       " 'أهلا',\n",
       " 'أيضا',\n",
       " 'بؤسا',\n",
       " 'بعدا',\n",
       " 'بغتة',\n",
       " 'تعسا',\n",
       " 'حقا',\n",
       " 'حمدا',\n",
       " 'خلافا',\n",
       " 'خاصة',\n",
       " 'دواليك',\n",
       " 'سحقا',\n",
       " 'سرا',\n",
       " 'سمعا',\n",
       " 'صبرا',\n",
       " 'صدقا',\n",
       " 'صراحة',\n",
       " 'طرا',\n",
       " 'عجبا',\n",
       " 'عيانا',\n",
       " 'غالبا',\n",
       " 'فرادى',\n",
       " 'فضلا',\n",
       " 'قاطبة',\n",
       " 'كثيرا',\n",
       " 'لبيك',\n",
       " 'معاذ',\n",
       " 'أبدا',\n",
       " 'إزاء',\n",
       " 'أصلا',\n",
       " 'الآن',\n",
       " 'أمد',\n",
       " 'أمس',\n",
       " 'آنفا',\n",
       " 'آناء',\n",
       " 'أنّى',\n",
       " 'أول',\n",
       " 'أيّان',\n",
       " 'تارة',\n",
       " 'ثمّ',\n",
       " 'ثمّة',\n",
       " 'حقا',\n",
       " 'صباح',\n",
       " 'مساء',\n",
       " 'ضحوة',\n",
       " 'عوض',\n",
       " 'غدا',\n",
       " 'غداة',\n",
       " 'قطّ',\n",
       " 'كلّما',\n",
       " 'لدن',\n",
       " 'لمّا',\n",
       " 'مرّة',\n",
       " 'قبل',\n",
       " 'خلف',\n",
       " 'أمام',\n",
       " 'فوق',\n",
       " 'تحت',\n",
       " 'يمين',\n",
       " 'شمال',\n",
       " 'ارتدّ',\n",
       " 'استحال',\n",
       " 'أصبح',\n",
       " 'أضحى',\n",
       " 'آض',\n",
       " 'أمسى',\n",
       " 'انقلب',\n",
       " 'بات',\n",
       " 'تبدّل',\n",
       " 'تحوّل',\n",
       " 'حار',\n",
       " 'رجع',\n",
       " 'راح',\n",
       " 'صار',\n",
       " 'ظلّ',\n",
       " 'عاد',\n",
       " 'غدا',\n",
       " 'كان',\n",
       " 'ما انفك',\n",
       " 'ما برح',\n",
       " 'مادام',\n",
       " 'مازال',\n",
       " 'مافتئ',\n",
       " 'ابتدأ',\n",
       " 'أخذ',\n",
       " 'اخلولق',\n",
       " 'أقبل',\n",
       " 'انبرى',\n",
       " 'أنشأ',\n",
       " 'أوشك',\n",
       " 'جعل',\n",
       " 'حرى',\n",
       " 'شرع',\n",
       " 'طفق',\n",
       " 'علق',\n",
       " 'قام',\n",
       " 'كرب',\n",
       " 'كاد',\n",
       " 'هبّ']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccdca6f7-60f7-41a8-9cb8-0ff2a9640739",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6660f9-2c42-481c-8d28-53e1df5f2953",
   "metadata": {},
   "source": [
    "### Stemming Does not Give a good result, So instead of Stemming apply Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b070d80d-e38c-4c11-b0e7-2eb9db3291eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4b4badd-be5c-45bb-81c7-1ab9fee1c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73c4b65e-cc69-4b77-bd31-7e7c844d5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82f3b456-40df-4044-8b06-a5ebaeebaba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Word Tokenization Remove Stop words and the apply stemming/Lemmatization\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    # sentences[i]=sentences[i].lower()\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmatization.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=\" \".join(words) # Joins words into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7a25dd2-df1a-4dc1-ab82-536ee38c5145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in word , i part ancient legacy stretch back dawn human civilization .',\n",
       " 'the great stride make together testament unyielding spirit , thrive overcome obstacles unite adversity .',\n",
       " 'however , journey without toll ; ’ reminder far ’ come yet much remain build inclusive prosperous future .',\n",
       " 'the challenge face today require strength hope .',\n",
       " 'the struggle areas like poverty , education , infrastructure continue stand reminders human condition—a struggle equality justice strive honor ignore .',\n",
       " 'yet , unity share purpose overcome obstacles .',\n",
       " 'this narrative resilience remind us , despite difficulties , still hope .',\n",
       " 'the spirit hope cultivate today continue inspire around us , foster sense unity camaraderie .',\n",
       " 'together , stand brink new opportunities—opportunity progress , opportunity inspiration—and potential better future .',\n",
       " 'as move forward , let us keep mind seed change plant every individual ’ hearts .',\n",
       " 'together , create future unity reign supreme hope prevail strength .',\n",
       " 'let us carry vision do—her march band , school , even streets cities—to inspire others join ongoing struggle better world .']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22f6ab-e32b-42e6-bd11-9fcfb5a62263",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a427163-9a41-49d5-84e1-2ae27847c6d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '“' (U+201C) (3667887647.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[48], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    EX existential there (like: “there is” … think of it like “there exists”)\u001b[0m\n\u001b[1;37m                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '“' (U+201C)\n"
     ]
    }
   ],
   "source": [
    "CC coordinating conjunction\n",
    "CD cardinal digit\n",
    "DT determiner\n",
    "EX existential there (like: “there is” … think of it like “there exists”)\n",
    "FW foreign word\n",
    "IN preposition/subordinating conjunction\n",
    "JJ adjective ‘big’\n",
    "JJR adjective, comparative ‘bigger’\n",
    "JJS adjective, superlative ‘biggest’\n",
    "LS list marker 1)\n",
    "MD modal could, will\n",
    "NN noun, singular ‘desk’\n",
    "NNS noun plural ‘desks’\n",
    "NNP proper noun, singular ‘Harrison’\n",
    "NNPS proper noun, plural ‘Americans’\n",
    "PDT predeterminer ‘all the kids’\n",
    "POS possessive ending parent’s\n",
    "PRP personal pronoun I, he, she\n",
    "PRP$ possessive pronoun my, his, hers\n",
    "RB adverb very, silently,\n",
    "RBR adverb, comparative better\n",
    "RBS adverb, superlative best\n",
    "RP particle give up\n",
    "TO, to go ‘to’ the store.\n",
    "UH interjection, errrrrrrrm\n",
    "VB verb, base form take\n",
    "VBD verb, past tense took\n",
    "VBG verb, gerund/present participle taking\n",
    "VBN verb, past participle taken\n",
    "VBP verb, sing. present, non-3d take\n",
    "VBZ verb, 3rd person sing. present takes\n",
    "WDT wh-determiner which\n",
    "WP wh-pronoun who, what\n",
    "WP$ possessive wh-pronoun whose\n",
    "WRB wh-abverb where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ee0b2ee-9fa7-4217-8554-7dc1ded98a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8c1b36cb-e459-4cf7-80ef-eec09207be4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (634996505.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[50], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    from\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67a98026-abe8-4bb0-b4c9-7c3707649a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35697268-6465-481d-98a5-51d59642f3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf29cbc2-b82e-47ec-bec3-06d47ba75124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('words', 'NNS'), (',', ','), ('I', 'PRP'), ('part', 'NN'), ('ancient', 'NN'), ('legacy', 'NN'), ('stretches', 'VBZ'), ('back', 'RB'), ('dawn', 'JJ'), ('human', 'JJ'), ('civilization', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('great', 'JJ'), ('strides', 'NNS'), ('made', 'VBN'), ('together', 'RB'), ('testament', 'JJ'), ('unyielding', 'VBG'), ('spirit', 'NN'), (',', ','), ('thrived', 'VBD'), ('overcoming', 'VBG'), ('obstacles', 'NNS'), ('uniting', 'VBG'), ('adversity', 'NN'), ('.', '.')]\n",
      "[('However', 'RB'), (',', ','), ('journey', 'NN'), ('without', 'IN'), ('toll', 'NN'), (';', ':'), ('’', 'CC'), ('reminder', 'VB'), ('far', 'RB'), ('’', 'JJ'), ('come', 'VBP'), ('yet', 'RB'), ('much', 'JJ'), ('remains', 'VBZ'), ('build', 'JJ'), ('inclusive', 'JJ'), ('prosperous', 'JJ'), ('future', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('challenges', 'NNS'), ('face', 'VBP'), ('today', 'NN'), ('require', 'VBP'), ('strength', 'NN'), ('hope', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('struggles', 'NNS'), ('areas', 'NNS'), ('like', 'IN'), ('poverty', 'NN'), (',', ','), ('education', 'NN'), (',', ','), ('infrastructure', 'NN'), ('continue', 'VBP'), ('stand', 'NN'), ('reminders', 'NNS'), ('human', 'JJ'), ('condition—a', 'VBP'), ('struggle', 'NN'), ('equality', 'NN'), ('justice', 'NN'), ('strive', 'JJ'), ('honor', 'NN'), ('ignore', 'NN'), ('.', '.')]\n",
      "[('Yet', 'RB'), (',', ','), ('unity', 'NN'), ('shared', 'VBD'), ('purpose', 'JJ'), ('overcome', 'JJ'), ('obstacles', 'NNS'), ('.', '.')]\n",
      "[('This', 'DT'), ('narrative', 'JJ'), ('resilience', 'NN'), ('reminds', 'VBZ'), ('us', 'PRP'), (',', ','), ('despite', 'IN'), ('difficulties', 'NNS'), (',', ','), ('still', 'RB'), ('hope', 'VBP'), ('.', '.')]\n",
      "[('The', 'DT'), ('spirit', 'NN'), ('hope', 'NN'), ('cultivate', 'NN'), ('today', 'NN'), ('continues', 'VBZ'), ('inspire', 'VBP'), ('around', 'IN'), ('us', 'PRP'), (',', ','), ('fostering', 'VBG'), ('sense', 'NN'), ('unity', 'NN'), ('camaraderie', 'NN'), ('.', '.')]\n",
      "[('Together', 'RB'), (',', ','), ('stand', 'VB'), ('brink', 'VB'), ('new', 'JJ'), ('opportunities—opportunity', 'NN'), ('progress', 'NN'), (',', ','), ('opportunity', 'NN'), ('inspiration—and', 'VBP'), ('potential', 'JJ'), ('better', 'JJR'), ('future', 'NN'), ('.', '.')]\n",
      "[('As', 'IN'), ('move', 'NN'), ('forward', 'RB'), (',', ','), ('let', 'VB'), ('us', 'PRP'), ('keep', 'VB'), ('mind', 'NN'), ('seeds', 'NNS'), ('change', 'VBP'), ('planted', 'VBN'), ('every', 'DT'), ('individual', 'NN'), ('’', 'NN'), ('hearts', 'NNS'), ('.', '.')]\n",
      "[('Together', 'RB'), (',', ','), ('create', 'VB'), ('future', 'JJ'), ('unity', 'NN'), ('reigns', 'NNS'), ('supreme', 'VBP'), ('hope', 'NN'), ('prevails', 'VBZ'), ('strength', 'NN'), ('.', '.')]\n",
      "[('Let', 'VB'), ('us', 'PRP'), ('carry', 'VB'), ('vision', 'NN'), ('do—her', 'RB'), ('marching', 'VBG'), ('bands', 'NNS'), (',', ','), ('schools', 'NNS'), (',', ','), ('even', 'RB'), ('streets', 'NNS'), ('cities—to', 'VBP'), ('inspire', 'NN'), ('others', 'NNS'), ('join', 'VBP'), ('ongoing', 'VBG'), ('struggle', 'NN'), ('better', 'RBR'), ('world', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36da194-e7e8-49b2-82da-f87f2b6982bf",
   "metadata": {},
   "source": [
    "## pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2473d22f-7df9-4efc-95d5-ea12616e71f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tokens: expected a list of strings, got a string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nltk\u001b[38;5;241m.\u001b[39mpos_tag_sents(nltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTaj mahal is a beaultiful Monument\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:184\u001b[0m, in \u001b[0;36mpos_tag_sents\u001b[1;34m(sentences, tagset, lang)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to tag the\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mgiven list of sentences, each consisting of a list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m:rtype: list(list(tuple(str, str)))\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_pos_tag(sent, tagset, tagger, lang) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:184\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to tag the\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mgiven list of sentences, each consisting of a list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m:rtype: list(list(tuple(str, str)))\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    183\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_pos_tag(sent, tagset, tagger, lang) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:120\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens: expected a list of strings, got a string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     tagged_tokens \u001b[38;5;241m=\u001b[39m tagger\u001b[38;5;241m.\u001b[39mtag(tokens)\n",
      "\u001b[1;31mTypeError\u001b[0m: tokens: expected a list of strings, got a string"
     ]
    }
   ],
   "source": [
    "nltk.pos_tag_sents(nltk.word_tokenize(\"Taj mahal is a beaultiful Monument\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5f003-9341-4ea0-ad48-e0c8c70fd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag_sents([nltk.word_tokenize(\"Taj mahal is a beaultiful Monument\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4f981-a222-4634-8d08-9a2a6730e6e7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# NAmed Entity Relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a061aa3-b382-4fbf-a092-afacddeeb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"The Eiffel Tower was built from 1887 to 1889 by French engineer Gustave Eiffel, whose company specialized in building metal frameworks and structures.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f182a35-450b-4987-b7f7-308b9fef2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d5dddafc-8b84-41df-a205-ce972eb23308",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag_words=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2031ed0-d032-4b2e-86a9-dab12ac933c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\karan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d59b55f7-6ea3-46b2-878a-d35cff13a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(pos_tag_words).draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff0067-486c-4a9a-9e02-ab3db5c7a159",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c4a1a-a6bf-4d67-9030-c6e745059153",
   "metadata": {},
   "source": [
    "## Word2Vec Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da67d024-845b-4618-b8e5-329a022d7f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\karan\\anaconda3\\lib\\site-packages (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23f4e0d0-7ba2-496e-bd2a-678fb7113271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\karan\\anaconda3\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: FuzzyTM>=0.4.0 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from gensim) (2.0.9)\n",
      "Requirement already satisfied: pandas in c:\\users\\karan\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (1.5.3)\n",
      "Requirement already satisfied: pyfume in c:\\users\\karan\\anaconda3\\lib\\site-packages (from FuzzyTM>=0.4.0->gensim) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from pandas->FuzzyTM>=0.4.0->gensim) (2023.3.post1)\n",
      "Requirement already satisfied: simpful==2.12.0 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from pyfume->FuzzyTM>=0.4.0->gensim) (1.8.1)\n",
      "Requirement already satisfied: miniful in c:\\users\\karan\\anaconda3\\lib\\site-packages (from fst-pso==1.8.1->pyfume->FuzzyTM>=0.4.0->gensim) (0.0.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\karan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->FuzzyTM>=0.4.0->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6076555a-19d5-4e5d-985c-ca609f2cae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400bca28-795a-4c4f-a1bc-a6da7a90e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bfb052-dca5-4161-ac1b-1eb63014eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.5% 291.7/1662.8MB downloaded"
     ]
    }
   ],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# w2v=api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# vec_king=w2v['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae900a-aea5-45a2-920d-0bb3b8121009",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9315a-a208-4d6d-aac9-52ac8a352299",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347db12f-5f5a-4466-8905-fa244a1fd758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
